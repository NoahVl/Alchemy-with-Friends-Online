{
    "blackCards": [
        {"text": "_____ should be trivial to implement."},
        {"text": "_____ is left as an exercise for the reader."},
        {"text": "I'm too important for _____."},
        {"text": "PhD students these days have no respect for _____."},
        {"text": "PhD advisors these days have no respect for _____."},
        {"text": "ML is all about _____."},
        {"text": "When I grow up, I will work on _____."},
        {"text": "The new review process optimizes for _____."},
        {"text": "I had the time of my life solving problems with _____."},
        {"text": "Blood, sweat, and _____."},
        {"text": "My supervisor could not respond to my email because apparently _____."},
        {"text": "I can't believe I had a paper with _____ accepted at NeurIPS."},
        {"text": "Hey kids, study ML and one day you too can aspire to work on _____."},
        {"text": "_____ is a slippery slope that leads to _____.", "pick": 2},
        {"text": "I finally experienced _____ and now I understand _____.", "pick": 2},
        {"text": "Self-Attention is so 2020. 2021 is all about _____."},
        {"text": "When I sell my AI start-up. I will erect a 20ft statue to commemorate _____."},
        {"text": "I told the AI recruiter I focus on _____."},
        {"text": "Reviewer 2 rejected the paper because of _____."},
        {"text": "The field of AI is stuck until we understand _____."},
        {"text": "During my PhD I focussed too much on _____."},
        {"text": "_____ explains how the brain works."},
        {"text": "_____ is the new electricity."},
        {"text": "_____ is all you need."},
        {"text": "_____ is a real ethical concern."},
        {"text": "Being Bayesian about _____ would solve all your problems."},
        {"text": "Neural _____ models are the next big thing."},
        {"text": "\"Science is _____\nReligion is _____.\"\n\n- Alan Turing",
            "pick": 2},
        {"text": "\"The development of _____ could spell the end of the human race.\"\n\n- Stephen Hawking"},
        {"text": "Deep _____."},
        {"text": "_____ is the largest model trained yet."},
        {"text": "_____ is just an attention mechanism."},
        {"text": "Deep Generative Models are all about _____."},
        {"text": "Training _____ is easy; just use _____.", "pick": 2},
        {"text": "_____ achieves superhuman performance."},
        {"text": "My start-up applies _____ to solve _____.", "pick": 2},
        {"text": "Can you believe that they consider _____ as a hyperparameter?", "pick": 2},
        {"text": "Artificial General Intelligence will be solved by _____."},
        {"text": "_____ is the problem, _____ is the solution.", "pick": 2},
        {"text": "All I want in life is _____."},
        {"text": "Anyone else having problems with _____ today, or is it just me?"},
        {"text": "Your baseline experiment should include _____."},
        {"text": "Modern Deep Learning is all about _____."},
        {"text": "_____ is essentially solved."},
        {"text": "Computer Vision is just _____."},
        {"text": "You can't have a CVPR party without _____."},
        {"text": "Computer Vision students dream of _____."},
        {"text": "I just trained a ConvNet to  _____."},
        {"text": "_____ is just _____.", "pick": 2},
        {"text": "_____ is truly visionary."},
        {"text": "_____ is harder than MNIST."},
        {"text": "_____ is a definite accept at CVPR."}
    ],
    "whiteCards": [
        "Schmidhuber",
        "Mode collapse",
        "Saddle points",
        "Reviewer #2",
        "Being scooped",
        "Blockchain",
        "Alchemy",
        "Real science",
        "Rigor police",
        "My NeurIPS paper",
        "Being a true Bayesian",
        "A debate about PyTorch vs TensorFlow",
        "Dropout",
        "Adding skip connections",
        "@ML_Hipster",
        "Tweeting @ BoredYannLecun",
        "Spending time on /r/ML",
        "Saying how much better NeurIPS was in the old days",
        "Debating when the singularity will happen",
        "#FeelTheLearn",
        "Testing on the training set",
        "AGI",
        "Not trying the linear baseline",
        "Hyperparameter optimisation (including the random seed)",
        "Reducing internal covariance shift",
        "Vacuous generalization bounds",
        "152 deep layers",
        "Exploding gradients",
        "Procrastinating while waiting for my GPU jobs to complete",
        "Temporally recurrent optimal learning",
        "Self-attention",
        "Gradient noise",
        "Collecting cool swag at NeurIPS",
        "Talking to AI recruiters",
        "Grad student descent",
        "Not plotting error bars",
        "Meta-meta learning",
        "Adversaries, bandits, and critics",
        "Carefully decaying the learning rate until it works",
        "Nature and Science papers",
        "Minimum innovation for maximum impact",
        "Citing the Canadian mafia",
        "#RocketAI",
        "Waiting for the winter to come",
        "Fixing random seeds",
        "Comparing to random search",
        "Ignoring the rebuttal period",
        "Putting it on arXiv first",
        "Montezuma's Revenge",
        "Simpson's paradox",
        "Using -999 to encode a missing value",
        "Outlier removal",
        "Restricted Boltzmann machines",
        "Gaining half a percent accuracy on CIFAR-10",
        "Some trivial data pre-processing",
        "The minimal viable product",
        "Sending complaint emails to the personal email address of the area chair",
        "Approximating AIXI",
        "My AI start-up",
        "Buying my NeurIPS registration on the darknet",
        "Occam's Razor",
        "Bits back",
        "GPU compute",
        "A weak classifier",
        "Machine Learning Yearning",
        "Causal leakage",
        "Overfitting",
        "Biased data collection",
        "Data visualization",
        "A bag of bootstraps",
        "Hand-coded features",
        "NeurIPS registration queues",
        "VC funding",
        "Inductive biases",
        "Optimizing on the test set",
        "Shrinkage",
        "Random kitchen sinks",
        "Selecting k in k-means",
        "Getting away with not reporting runtimes in your paper",
        "Submitting your paper to a double-blind conference and arXiv at the same time",
        "Asking the author to cite five of your papers in your anonymous review",
        "An insufficient literature review",
        "Finding a bug in your code after submitting your paper",
        "p<0.5",
        "Cherry picking results",
        "Bugs as regularizers",
        "Sharing a single GPU with your lab mates",
        "Hacking the NeurIPS style file",
        "Flattering likely reviewers in the background work section",
        "The importance of convexity",
        "Local optima",
        "Random restarts",
        "Early stopping",
        "Gradient descent on the validation set",
        "Using sudo to remove other people's GPU lock",
        "A reproducibility crisis",
        "Classifying MNIST digit 2 vs 9",
        "Vapnik's principle",
        "Software 2.0",
        "Writing neural networks in Lua",
        "Doing backpropagation manually in numpy",
        "Ignoring ancient works pre-2015",
        "Referring to work as AI rather than ML",
        "Worrying that your data is not big enough",
        "Discussion of the growth in NeurIPS attendees",
        "Trying to shoehorn any problem through a ResNet",
        "Einsum",
        "Trying to come up with a catchy name for a new model",
        "Writing a paper on the day of the deadline because you finally beat the random baseline",
        "What conversational agents gossip with each other",
        "Competing with robots in an automated warehouse",
        "Hiring mechanical Turk workers to tune the hyperparameters of your RL agent",
        "Novel evaluation metric",
        "Math to look smart",
        "Understand the bug that made it work",
        "A conversational agent's social media addiction",
        "P(B | A) = P(A | B) P(B)/P(A)",
        "import torch",
        "Sparsity",
        "Moving sliders at http://distill.pub/",
        "Causality",
        "The prosecutorâ€™s fallacy",
        "Data missing not at random",
        "Checking arXiv obsessively",
        "Learning more math",
        "Differential privacy",
        "Dataset selection",
        "Unsupervised auxiliary tasks",
        "Catastrophic forgetting",
        "Non-parametric",
        "Skipping on a multiple testing correction",
        "Optimizing the most convenient objective",
        "Random noise",
        "A theorem",
        "Relational bias",
        "Mathiness",
        "The NeurIPS random ballot",
        "1000 GPUs",
        "50+ billion parameters",
        "Geometric Deep Learning",
        "OpenReview",
        "Meta-forgetting",
        "Elmo and Bert",
        "Deep learning celebrities",
        "Pretraining",
        "Differential programming",
        "Deep Learning for Dummies",
        "Xavier and/or Glorot",
        "Deep fakes",
        "Information bottleneck",
        "Gradient explosion",
        "A bigger, better Schmidhuber",
        "Plush giraffe perturbation",
        "Predatory ML bootcamps",
        "\uD83E\uDD17 Huggingface",
        "Sunday night deadlines",
        "Adversarial examples",
        "Learning on the Edge",
        "The Big Bird backronym problem",
        "6-bit floats",
        "HoloLens2",
        "Autonomous driving",
        "The Microsoft party",
        "Edge detection",
        "My CVPR paper",
        "My rejected CVPR paper",
        "Curve fitting",
        "ImageNet",
        "Caltech 101",
        "Facial recognition",
        "Cats and dogs",
        "Biological vision",
        "Vanilla CNN",
        "Fast R-CNN",
        "Object detection",
        "MNIST",
        "CIFAR",
        "GANs",
        "InstaGAN",
        "1x1 convolution",
        "ConvNet",
        "Adversarial learning",
        "Other computer vision conferences",
        "Causality",
        "The related work section",
        "My startup",
        "Computer vision and pattern recognition",
        "Semantic segmentation",
        "Lunch",
        "Interpretability",
        "Real world applications",
        "Transfer learning",
        "State of the art",
        "Artificial General Intelligence",
        "Negative results",
        "Causality"
    ]
}
