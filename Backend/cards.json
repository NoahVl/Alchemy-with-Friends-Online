{
    "blackCards": [
        {"text": "_____ should be trivial to implement."},
        {"text": "_____ is left as an exercise for the reader."},
        {"text": "I'm too important for _____."},
        {"text": "PhD students these days have no respect for _____."},
        {"text": "PhD advisors these days have no respect for _____."},
        {"text": "ML is all about _____."},
        {"text": "When I grow up, I will work on _____."},
        {"text": "The new review process optimizes for _____."},
        {"text": "I had the time of my life solving problems with _____."},
        {"text": "Blood, sweat, and _____."},
        {"text": "My supervisor could not respond to my email because apparently _____."},
        {"text": "I can't believe I had a paper with _____ accepted at NeurIPS."},
        {"text": "Hey kids, study ML and one day you too can aspire to work on _____."},
        {"text": "_____ is a slippery slope that leads to _____.", "pick": 2},
        {"text": "I finally experienced _____ and now I understand _____.", "pick": 2},
        {"text": "Self-Attention is so 2020. 2021 is all about _____."},
        {"text": "When I sell my AI start-up. I will erect a 20ft statue to commemorate _____."},
        {"text": "I told the AI recruiter I focus on _____."},
        {"text": "Reviewer 2 rejected the paper because of _____."},
        {"text": "The field of AI is stuck until we understand _____."},
        {"text": "During my PhD I focussed too much on _____."},
        {"text": "_____ explains how the brain works."},
        {"text": "_____ is the new electricity."},
        {"text": "_____ is all you need."},
        {"text": "_____ is a real ethical concern."},
        {"text": "Being Bayesian about _____ would solve all your problems."},
        {"text": "Neural _____ models are the next big thing."},
        {"text": "\"Science is _____\nReligion is _____.\"\n\n- Alan Turing",
            "pick": 2},
        {"text": "\"The development of _____ could spell the end of the human race.\"\n\n- Stephen Hawking"}
    ],
    "whiteCards": [
        "Schmidhuber",
        "Mode collapse",
        "Saddle points",
        "Reviewer #2",
        "Being scooped",
        "Blockchain",
        "Alchemy",
        "Real science",
        "Rigor police",
        "My NeurIPS paper",
        "Being a true Bayesian",
        "A debate about PyTorch vs TensorFlow",
        "Dropout",
        "Adding skip connections",
        "@ML_Hipster",
        "Tweeting @ BoredYannLecun",
        "Spending time on /r/ML",
        "Saying how much better NeurIPS was in the old days",
        "Debating when the singularity will happen",
        "#FeelTheLearn",
        "Testing on the training set",
        "AGI",
        "Not trying the linear baseline",
        "Hyperparameter optimisation (including the random seed)",
        "Reducing internal covariance shift",
        "Vacuous generalization bounds",
        "152 deep layers",
        "Exploding gradients",
        "Procrastinating while waiting for my GPU jobs to complete",
        "Temporally recurrent optimal learning",
        "Self-attention",
        "Gradient noise",
        "Collecting cool swag at NeurIPS",
        "Talking to AI recruiters",
        "Grad student descent",
        "Not plotting error bars",
        "Meta-meta learning",
        "Adversaries, bandits, and critics",
        "Carefully decaying the learning rate until it works",
        "Nature and Science papers",
        "Minimum innovation for maximum impact",
        "Citing the Canadian mafia",
        "#RocketAI",
        "Waiting for the winter to come",
        "Fixing random seeds",
        "Comparing to random search",
        "Ignoring the rebuttal period",
        "Putting it on arXiv first",
        "Montezuma's Revenge",
        "Simpson's paradox",
        "Using -999 to encode a missing value",
        "Outlier removal",
        "Restricted Boltzmann machines",
        "Gaining half a percent accuracy on CIFAR-10",
        "Some trivial data pre-processing",
        "The minimal viable product",
        "Sending complaint emails to the personal email address of the area chair",
        "Approximating AIXI",
        "My AI start-up",
        "Buying my NeurIPS registration on the darknet",
        "Occam's Razor",
        "Bits back",
        "GPU compute",
        "A weak classifier",
        "Machine Learning Yearning",
        "Causal leakage",
        "Overfitting",
        "Biased data collection",
        "Data visualization",
        "A bag of bootstraps",
        "Hand-coded features",
        "NeurIPS registration queues",
        "VC funding",
        "Inductive biases",
        "Optimizing on the test set",
        "Shrinkage",
        "Random kitchen sinks",
        "Selecting k in k-means",
        "Getting away with not reporting runtimes in your paper",
        "Submitting your paper to a double-blind conference and arXiv at the same time",
        "Asking the author to cite five of your papers in your anonymous review",
        "An insufficient literature review",
        "Finding a bug in your code after submitting your paper",
        "p<0.5",
        "Cherry picking results",
        "Bugs as regularizers",
        "Sharing a single GPU with your lab mates",
        "Hacking the NeurIPS style file",
        "Flattering likely reviewers in the background work section",
        "The importance of convexity",
        "Local optima",
        "Random restarts",
        "Early stopping",
        "Gradient descent on the validation set",
        "Using sudo to remove other people's GPU lock",
        "A reproducibility crisis",
        "Classifying MNIST digit 2 vs 9",
        "Vapnik's principle",
        "Software 2.0",
        "Writing neural networks in Lua",
        "Doing backpropagation manually in numpy",
        "Ignoring ancient works pre-2015",
        "Referring to work as AI rather than ML",
        "Worrying that your data is not big enough",
        "Discussion of the growth in NeurIPS attendees",
        "Trying to shoehorn any problem through a ResNet",
        "Einsum",
        "Trying to come up with a catchy name for a new model",
        "Writing a paper on the day of the deadline because you finally beat the random baseline",
        "What conversational agents gossip with each other",
        "Competing with robots in an automated warehouse",
        "Hiring mechanical Turk workers to tune the hyperparameters of your RL agent",
        "Novel evaluation metric",
        "Math to look smart",
        "Understand the bug that made it work",
        "A conversational agent's social media addiction",
        "P(B | A) = P(A | B) P(B)/P(A)",
        "import torch"
    ]
}
